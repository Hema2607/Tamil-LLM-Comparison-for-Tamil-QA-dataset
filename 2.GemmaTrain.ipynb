{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d9dc36-f702-44ee-9ca4-bde52e7f6c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.6.3-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.19.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting unsloth_zoo>=2025.6.2 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.6.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.0a0+e000cf0ad9.nv24.10)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (23.2)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.24-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 (from unsloth)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.24.4)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.24.4)\n",
      "Collecting huggingface_hub (from unsloth)\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Downloading diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.20.0a0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub->unsloth)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<=2.7.0,>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth) (2024.9.11)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton>=3.0.0->unsloth) (70.3.0)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.6.2->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.6.2->unsloth) (10.4.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.6.2->unsloth)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting torch<=2.7.0,>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting sympy>=1.13.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<=2.7.0,>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->unsloth) (7.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.22.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->unsloth)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->unsloth) (3.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.7.0,>=2.4.0->unsloth) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
      "Downloading unsloth-2025.6.3-py3-none-any.whl (277 kB)\n",
      "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading trl-0.19.0-py3-none-any.whl (375 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.6.2-py3-none-any.whl (149 kB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.30-cp310-cp310-manylinux_2_28_x86_64.whl (31.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.24-py3-none-any.whl (128 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Installing collected packages: sentencepiece, nvidia-cusparselt-cu12, xxhash, typing-extensions, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf-xet, hf_transfer, docstring-parser, dill, typeguard, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, huggingface_hub, tyro, tokenizers, nvidia-cusolver-cu12, diffusers, transformers, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, accelerate, trl, peft, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.4\n",
      "    Uninstalling protobuf-4.24.4:\n",
      "      Successfully uninstalled protobuf-4.24.4\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.22.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.22.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.22.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0a0+e000cf0ad9.nv24.10\n",
      "    Uninstalling torch-2.5.0a0+e000cf0ad9.nv24.10:\n",
      "      Successfully uninstalled torch-2.5.0a0+e000cf0ad9.nv24.10\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.0a0\n",
      "    Uninstalling torchvision-0.20.0a0:\n",
      "      Successfully uninstalled torchvision-0.20.0a0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.5.0a0 requires torch<2.6.0,>=2.5.0.dev, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.8.1 bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.33.1 dill-0.3.8 docstring-parser-0.16 hf-xet-1.1.5 hf_transfer-0.1.9 huggingface_hub-0.33.0 msgspec-0.19.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 peft-0.15.2 protobuf-3.20.3 sentencepiece-0.2.0 shtab-1.7.2 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 torchvision-0.22.0 transformers-4.52.4 triton-3.3.0 trl-0.19.0 typeguard-4.4.4 typing-extensions-4.14.0 tyro-0.9.24 unsloth-2025.6.3 unsloth_zoo-2025.6.2 xformers-0.0.30 xxhash-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1. Install dependencies\n",
    "!pip install -U unsloth accelerate peft bitsandbytes datasets trl sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d378e7a-4718-4675-9740-af3b29b17ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, upgrade flash-attn to version 2.6.3 or higher!\n",
      "Newer versions support faster and less memory usage kernels for Gemma 2's attention softcapping!\n",
      "To update flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "# ✅ 2. Imports & Setup\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d2bba0-62a5-4c88-bd90-5a85fa520c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 3. Load and format the dataset\n",
    "df = pd.read_csv(\"medquad_tamil.csv\").dropna()\n",
    "\n",
    "# ✅ Format prompt as: ### கேள்வி:\\n...\\n\\n### பதில்:\\n...\n",
    "df[\"text\"] = \"### கேள்வி:\\n\" + df[\"question_tamil\"] + \"\\n\\n### பதில்:\\n\" + df[\"answer_tamil\"]\n",
    "\n",
    "# ✅ Split 90:10 and retain all columns\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# ✅ Convert to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59bbea07-66fd-42c7-beb4-8a54b81ffd2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Gemma patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# ✅ 4. Load model from Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"abhinand/gemma-2b-it-tamil-v0.1-alpha\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.float16,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e832546-4341-4236-84d0-d10e5828f308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.6.2 patched 18 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 5. Add LoRA (efficient fine-tuning)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98236ab8-603f-43b4-9fd6-5d79ffa5ba90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14766/14766 [00:05<00:00, 2798.27 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1641/1641 [00:00<00:00, 2888.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ✅ 6. Preprocessing\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6de25da-c06e-47d4-928c-eec58e9fab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 7. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"tamil_gemma_medqa\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",  # Set to \"wandb\" if using Weights & Biases\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72d1767-9514-42cd-957a-f5c2c4c5c987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 14,766 | Num Epochs = 5 | Total steps = 4,615\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 19,611,648/2,000,000,000 (0.98% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4615' max='4615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4615/4615 2:59:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.170500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.155200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.155500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.146300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.145400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.150900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.153700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.149100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.143700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.140400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.142300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.135800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.133700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>0.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>0.130100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>0.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>0.131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>0.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>0.119100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>0.119000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>0.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>0.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>0.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.116500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>0.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>0.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>0.118400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>0.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>0.126100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>0.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>0.131400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>0.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4310</td>\n",
       "      <td>0.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4330</td>\n",
       "      <td>0.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4370</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4390</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4430</td>\n",
       "      <td>0.130700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4470</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4490</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4510</td>\n",
       "      <td>0.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4530</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>0.124200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.134600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4570</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4590</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4610</td>\n",
       "      <td>0.114900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8a4a4fd4-03d0-4eb3-9db1-e361ce514f3f)') - silently ignoring the lookup for the file config.json in abhinand/gemma-2b-it-tamil-v0.1-alpha.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in abhinand/gemma-2b-it-tamil-v0.1-alpha - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/abhinand/gemma-2b-it-tamil-v0.1-alpha/resolve/main/config.json - silently ignoring the lookup for the file config.json in abhinand/gemma-2b-it-tamil-v0.1-alpha.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in abhinand/gemma-2b-it-tamil-v0.1-alpha - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n",
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4615, training_loss=0.14329929647569667, metrics={'train_runtime': 10797.6511, 'train_samples_per_second': 6.838, 'train_steps_per_second': 0.427, 'total_flos': 9.079016718650573e+17, 'train_loss': 0.14329929647569667})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 8. Train with SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a322433c-b5af-4884-9dbc-76724f138606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace with the path to your trainer_state.json\n",
    "state_path = \"tamil_gemma_medqa/checkpoint-500/trainer_state.json\"\n",
    "\n",
    "with open(state_path) as f:\n",
    "    state = json.load(f)\n",
    "\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for log in state[\"log_history\"]:\n",
    "    if \"loss\" in log:\n",
    "        steps.append(log[\"step\"])\n",
    "        losses.append(log[\"loss\"])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, losses, marker='o')\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eccd1d42-4977-438f-8f88-c0b607597f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [1, 2], your vocabulary could be corrupted !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tamil_gemma_finetuned_medqa/tokenizer_config.json',\n",
       " 'tamil_gemma_finetuned_medqa/special_tokens_map.json',\n",
       " 'tamil_gemma_finetuned_medqa/tokenizer.model',\n",
       " 'tamil_gemma_finetuned_medqa/added_tokens.json',\n",
       " 'tamil_gemma_finetuned_medqa/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 9. Save the model\n",
    "trainer.model.save_pretrained(\"tamil_gemma_finetuned_medqa\")\n",
    "tokenizer.save_pretrained(\"tamil_gemma_finetuned_medqa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1006e4-8935-45c1-8f9c-76cce4358c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.7.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.52.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.24.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.0->torch>=1.0.0->bert-score) (70.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.33.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.8.30)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c9ae16a-d4e5-420f-b2b9-93ad2e154ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.2: Fast Gemma patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.394 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abhinand/gemma-2b-it-tamil-v0.1-alpha does not have a padding token! Will use pad_token = <pad>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1641/1641 [3:15:23<00:00,  7.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"tamil_gemma_finetuned_medqa\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# ✅ Prepare for evaluation\n",
    "generated_answers = []\n",
    "reference_answers = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for item in tqdm(test_dataset):\n",
    "    question = item[\"question_tamil\"]\n",
    "    reference = item[\"answer_tamil\"]\n",
    "\n",
    "    prompt = f\"### கேள்வி:\\n{question}\\n\\n### பதில்:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract generated answer part\n",
    "    if \"### பதில்:\\n\" in decoded:\n",
    "        answer = decoded.split(\"### பதில்:\\n\")[-1].strip()\n",
    "    else:\n",
    "        answer = decoded.strip()\n",
    "\n",
    "    generated_answers.append(answer)\n",
    "    reference_answers.append(reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7193388e-15f7-4970-b10d-dea7d7aeb888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6616\n",
      "Recall:    0.6493\n",
      "F1 Score:  0.6543\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score(generated_answers, reference_answers, lang=\"ta\")  # ta = Tamil\n",
    "\n",
    "print(f\"Precision: {P.mean().item():.4f}\")\n",
    "print(f\"Recall:    {R.mean().item():.4f}\")\n",
    "print(f\"F1 Score:  {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9993e1a7-80ac-4df2-97b7-2768a703f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ BERTScore Precision: 0.6616\n",
      "✅ BERTScore Recall:    0.6493\n",
      "✅ BERTScore F1:        0.6543\n",
      "✅ Saved: tamil_bertscore.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert tensors to numpy\n",
    "P = P.numpy()\n",
    "R = R.numpy()\n",
    "F1 = F1.numpy()\n",
    "\n",
    "# Report average\n",
    "print(f\"\\n✅ BERTScore Precision: {P.mean():.4f}\")\n",
    "print(f\"✅ BERTScore Recall:    {R.mean():.4f}\")\n",
    "print(f\"✅ BERTScore F1:        {F1.mean():.4f}\")\n",
    "\n",
    "# Optional: Save individual results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Generated Answer\": generated_answers,\n",
    "    \"Reference Answer\": reference_answers,\n",
    "    \"Precision\": P,\n",
    "    \"Recall\": R,\n",
    "    \"F1 Score\": F1\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"tamil_bertscore.csv\", index=False)\n",
    "print(\"✅ Saved: tamil_bertscore.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
